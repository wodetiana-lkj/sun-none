gradient descent
$$
\theta_{j} := \theta_{j} - \alpha\frac{\partial}{\partial\theta_{j}}J(\theta_{0},\theta_{1})
$$
梯度下降

θ<sub>0</sub>、θ<sub>1</sub>都必须simultaneous update

公式推导
$$
\theta_{0} := \theta_{0} - \alpha \frac{1}{m} \sum^{m}_{i=1}[h_{\theta}(x^{(i)}) - y^{(i)}] \cdot x^{(i)}_{0}
$$
梯度下降需要和正规方程横向比较