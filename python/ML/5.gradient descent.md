gradient descent
$$
\theta_{j} := \theta_{j} - \alpha\frac{\partial}{\partial\theta_{j}}J(\theta_{0},\theta_{1})
$$
梯度下降

θ<sub>0</sub>、θ<sub>1</sub>都必须simultaneous update

公式推导
$$
\theta_{0} := \theta_{0} - \alpha \frac{1}{m} \sum_{i=1}^{m}[h_{\theta}(x^{(i)}) - y^{(i)}] \cdot x^{(i)}
$$