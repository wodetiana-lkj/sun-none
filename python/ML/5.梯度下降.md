gradient descent
$$
\theta_{j} := \theta_{j} - \alpha\frac{\partial}{\partial\theta_{j}}J(\theta_{0},\theta_{1})
$$
梯度下降

θ<sub>0</sub>、θ<sub>1</sub>都必须simultaneous update